import os
from tqdm import tqdm
import pymupdf
import fitz
import traceback
import re
import unicodedata
import glob
from collections import defaultdict
from pathlib import Path

from langchain_docling import DoclingLoader
from langchain_docling.loader import ExportType
from typing import Annotated, List, TypedDict, Literal
from langgraph.graph.message import add_messages
from langchain_core.documents import Document

from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain_community.cross_encoders import HuggingFaceCrossEncoder


# PDFê°€ ìˆëŠ” í´ë”ì™€ ë³€í™˜ëœ Markdownì„ ì €ì¥í•  í´ë”
pdf_dir = "./architecturePDF"  # ê±´ì„¤ì•ˆì „ì§€ì¹¨ ë°ì´í„° ê²½ë¡œ
markdown_dir = "./markdown"

# Markdown í´ë” ìƒì„±
os.makedirs(markdown_dir, exist_ok=True)

# GPU ì•„í‚¤í…ì²˜ ì„¤ì •
os.environ['TORCH_CUDA_ARCH_LIST'] = "8.6"  # ì‚¬ìš© GPU ì•„í‚¤í…ì²˜ì— ë§ê²Œ ìˆ˜ì •

# PDF ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  PDF íŒŒì¼ ê°€ì ¸ì˜¤ê¸°
pdf_files = [f for f in os.listdir(pdf_dir) if f.endswith('.pdf')]

# ì˜¤ë¥˜ ë¡œê·¸ë¥¼ ì €ì¥í•  íŒŒì¼
error_log_path = os.path.join(".", "pdf_conversion_errors.log")


def improve_headers(markdown_text):
    """ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸ì—ì„œ í—¤ë” êµ¬ì¡°ë¥¼ ê°œì„ í•˜ëŠ” í•¨ìˆ˜"""
    # 1. ì¤„ ë‹¨ìœ„ë¡œ ë¶„í• 
    lines = markdown_text.split('\n')
    improved_lines = []

    # ìˆ«ì í˜•ì‹ íŒ¨í„´ - í—¤ë”ì¸ì§€ í™•ì¸í•˜ëŠ” ë° ì‚¬ìš©
    chapter_pattern = re.compile(r'^##\s+(\d+\.)\s+(.+)$')  # 1. ì œëª©
    section_pattern = re.compile(r'^##\s+(\d+\.\d+\.)\s+(.+)$')  # 1.1. ì œëª©
    subsection_pattern = re.compile(r'^##\s+(\d+\.\d+\.\d+\.)\s+(.+)$')  # 1.1.1. ì œëª©

    for line in lines:
        # í—¤ë” ìˆ˜ì¤€ ê°œì„ 
        if line.startswith('## '):
            # ì±•í„° (1. ì œëª©)
            chapter_match = chapter_pattern.match(line)
            if chapter_match:
                improved_lines.append(f"# {chapter_match.group(1)} {chapter_match.group(2)}")
                continue

            # ì„¹ì…˜ (1.1. ì œëª©)
            section_match = section_pattern.match(line)
            if section_match:
                improved_lines.append(f"## {section_match.group(1)} {section_match.group(2)}")
                continue

            # ì„œë¸Œì„¹ì…˜ (1.1.1. ì œëª©)
            subsection_match = subsection_pattern.match(line)
            if subsection_match:
                improved_lines.append(f"### {subsection_match.group(1)} {subsection_match.group(2)}")
                continue

            # íŠ¹ìˆ˜ í—¤ë” ì‹ë³„ (ëª¨ë‘ ëŒ€ë¬¸ìì´ê±°ë‚˜ íŠ¹ìˆ˜ í‚¤ì›Œë“œ í¬í•¨)
            text = line[3:].strip()
            if text.isupper() or any(keyword in text for keyword in ['ëª©ì ', 'ì ìš©ë²”ìœ„', 'ìš©ì–´ì •ì˜', 'ì°¸ê³ ë¬¸í—Œ', 'ë¶€ë¡']):
                improved_lines.append(f"# {text}")
            elif len(text) < 50 and not text.endswith('.'):  # ì§§ì€ ë¬¸ì¥ì´ê³  ë§ˆì¹¨í‘œë¡œ ëë‚˜ì§€ ì•Šìœ¼ë©´ ë³´í†µ ì œëª©
                improved_lines.append(f"## {text}")
            else:
                improved_lines.append(line)  # ê¸°ì¡´ í—¤ë” ìœ ì§€
        else:
            improved_lines.append(line)

    return '\n'.join(improved_lines)


# tqdmì„ ì‚¬ìš©í•˜ì—¬ ì§„í–‰ ìƒí™© í‘œì‹œ
for pdf_file in tqdm(pdf_files, desc="PDF transform", unit="file"):
    pdf_path = os.path.join(pdf_dir, pdf_file)

    try:
        # LangChain DoclingLoaderë¡œ PDF -> Markdown ë³€í™˜
        loader = DoclingLoader(
            file_path=pdf_path,
            export_type=ExportType.MARKDOWN
        )
        docs = loader.load()

        # ë§ˆí¬ë‹¤ìš´ ë‚´ìš© ìˆ˜ì§‘ ë° ê°œì„ 
        improved_content = ""
        for doc in docs:
            # ê° ë¬¸ì„œ ë‚´ìš© ê°œì„ 
            improved_content += improve_headers(doc.page_content) + "\n\n"

        # íŒŒì¼ ì´ë¦„ì—ì„œ í™•ì¥ìë¥¼ ì œì™¸í•œ ë¶€ë¶„ì„ ì œëª©ìœ¼ë¡œ ì‚¬ìš©
        title = os.path.splitext(pdf_file)[0]
        final_content = f"# {title}\n\n{improved_content}"

        # ë³€í™˜ëœ í…ìŠ¤íŠ¸ ì €ì¥
        markdown_path = os.path.join(markdown_dir, pdf_file.replace('.pdf', '.md'))
        with open(markdown_path, "w", encoding="utf-8") as f:
            f.write(final_content)

    except Exception as e:
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¡œê·¸ íŒŒì¼ì— ê¸°ë¡
        with open(error_log_path, "a", encoding="utf-8") as error_file:
            error_file.write(f"Error converting {pdf_file}:\n")
            error_file.write(traceback.format_exc())
            error_file.write("\n" + "=" * 50 + "\n")
        print(f"Error converting {pdf_file}. See log for details.")

        # ì˜¤ë¥˜ê°€ ë°œìƒí•´ë„ ë¹ˆ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ìƒì„±
        markdown_path = os.path.join(markdown_dir, pdf_file.replace('.pdf', '.md'))
        with open(markdown_path, "w", encoding="utf-8") as f:
            f.write(f"# Conversion Error\n\nFailed to convert {pdf_file}. See error log for details.\n")

print("Conversion process completed")

# PDFê°€ ìˆëŠ” í´ë”ì™€ ë³€í™˜ëœ Markdownì„ ì €ì¥í•  í´ë”
pdf_dir = Path("./arc")  # ìƒëŒ€ ê²½ë¡œë¡œ ë³€ê²½
markdown_dir = Path("./markdown")
markdown_dir.mkdir(exist_ok=True)  # Markdown í´ë” ìƒì„±

# ì˜¤ë¥˜ ë¡œê·¸ íŒŒì¼
error_log_path = Path("./pdf_conversion_errors.log")

# ë¬¸ì œê°€ ìˆëŠ” íŒŒì¼ ëª©ë¡ (ì •ê·œí™” ì ìš©)
problematic_files = [
    "íƒ€ì›Œí¬ë ˆì¸ ì„¤ì¹˜, ì¡°ë¦½, í•´ì²´ ì‘ì—…ê³„íšì„œ ì‘ì„±ì§€ì¹¨.pdf",
    "ê°•ê´€ë¹„ê³„ ì•ˆì „ì‘ì—…ì§€ì¹¨.pdf",
    "íŒŒì´í”„ ì„œí¬íŠ¸ ë™ë°”ë¦¬ ì•ˆì „ì‘ì—… ì§€ì¹¨.pdf",
    "ì´ˆê³ ì¸µ ê±´ì¶•ë¬¼ê³µì‚¬(í™”ì¬ì˜ˆë°©) ì•ˆì „ë³´ê±´ì‘ì—…ì§€ì¹¨.pdf",
    "ì‹œìŠ¤í…œí¼(RCSí¼,ACSí¼ ì¤‘ì‹¬) ì•ˆì „ì‘ì—… ì§€ì¹¨.pdf",
    "F.C.M êµëŸ‰ê³µì‚¬ ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.pdf",
    "ì‹œìŠ¤í…œ ë™ë°”ë¦¬ ì•ˆì „ì‘ì—… ì§€ì¹¨.pdf",
    "ê¸ˆì† ì»¤íŠ¼ì›”(Curtain wall) ì•ˆì „ì‘ì—… ì§€ì¹¨.pdf",
    "íƒ€ê¸°, í•­ë°œê¸° ì‚¬ìš© ì‘ì—…ê³„íšì„œ ì‘ì„±ì§€ì¹¨.pdf",
    "íƒ‘ë‹¤ìš´(Top down) ê³µë²• ì•ˆì „ì‘ì—… ì§€ì¹¨.pdf",
    'ë¸”ë¡ì‹ ë³´ê°•í†  ì˜¹ë²½ ê³µì‚¬ ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.pdf'
]


# í•œê¸€ íŒŒì¼ëª… ì •ê·œí™” (NFC/NFD ë¬¸ì œ í•´ê²°)
def normalize_filename(filename, form="NFC"):
    return unicodedata.normalize(form, filename)


# PDFë¥¼ êµ¬ì¡°ì ì¸ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜ (ê¸°ì¡´ê³¼ ë™ì¼)
def convert_pdf_to_structured_markdown(pdf_path: Path, markdown_path: Path):
    doc = fitz.open(str(pdf_path))
    markdown_content = [f"# {pdf_path.stem}\n"]
    font_stats = defaultdict(int)
    font_sizes = []

    # ì²« ë²ˆì§¸ íŒ¨ìŠ¤: í°íŠ¸ í†µê³„ ìˆ˜ì§‘
    for page in doc:
        blocks = page.get_text("dict")["blocks"]
        for block in blocks:
            if "lines" in block:
                for line in block["lines"]:
                    for span in line["spans"]:
                        font_size = span["size"]
                        font_name = span["font"]
                        is_bold = "bold" in font_name.lower() or span["flags"] & 2 > 0
                        font_key = (font_size, is_bold)
                        font_stats[font_key] += 1
                        font_sizes.append(font_size)

    avg_font_size = sum(font_sizes) / len(font_sizes) if font_sizes else 11
    max_font_size = max(font_sizes) if font_sizes else 12

    # ë‘ ë²ˆì§¸ íŒ¨ìŠ¤: ë§ˆí¬ë‹¤ìš´ ë³€í™˜
    for page in doc:
        blocks = page.get_text("dict")["blocks"]
        current_block_text = []

        for block in blocks:
            if "lines" not in block:
                continue
            block_text = ""
            previous_y = None
            for line in block["lines"]:
                line_text = ""
                is_header = False
                font_size = 0
                is_bold = False

                for span in line["spans"]:
                    line_text += span["text"]
                    font_size = max(font_size, span["size"])
                    is_bold = is_bold or "bold" in span["font"].lower() or span["flags"] & 2 > 0

                if previous_y is not None and line["bbox"][1] - previous_y > 1.5 * avg_font_size:
                    block_text += "\n\n"
                elif previous_y is not None:
                    block_text += " "

                is_header = (
                        font_size > avg_font_size * 1.2 or
                        is_bold and font_size > avg_font_size * 1.1 or
                        font_size > max_font_size * 0.9 or
                        re.match(r"^\s*\d+(\.\d+)*\s+[A-Za-zê°€-í£]", line_text) or
                        re.match(r"^\s*[ì œì¥í•­ì ˆ]\s*\d+", line_text)
                )

                if is_header:
                    header_level = 2
                    if font_size > max_font_size * 0.95:
                        header_level = 2
                    elif font_size > avg_font_size * 1.5:
                        header_level = 3
                    elif font_size > avg_font_size * 1.2 or is_bold:
                        header_level = 4
                    else:
                        header_level = 5

                    if re.match(r"^\s*\d+\.\d+\.\d+", line_text):
                        header_level = 4
                    elif re.match(r"^\s*\d+\.\d+", line_text):
                        header_level = 3
                    elif re.match(r"^\s*\d+\s+", line_text) or re.match(r"^\s*[ì œì¥]\s*\d+", line_text):
                        header_level = 2

                    block_text += f"\n\n{'#' * header_level} {line_text.strip()}\n\n"
                else:
                    block_text += line_text

                previous_y = line["bbox"][3]

            if block_text.strip():
                current_block_text.append(block_text)

        if current_block_text:
            page_text = "\n".join(current_block_text)
            page_text = re.sub(r'\n{3,}', '\n\n', page_text)
            markdown_content.append(page_text)

    markdown_path.write_text("\n".join(markdown_content), encoding="utf-8")
    return True


# GLOB íŒ¨í„´ì„ ì‚¬ìš©í•˜ì—¬ problematic_filesì— ìˆëŠ” íŒŒì¼ë§Œ ì²˜ë¦¬
processed_files = []
for problem_file in tqdm(problematic_files, desc="Finding problematic files", unit="file"):
    file_base = problem_file.replace('.pdf', '')

    # NFCì™€ NFD ëª¨ë‘ ì‹œë„
    for norm_form in ["NFC", "NFD"]:
        normalized_file = normalize_filename(file_base, norm_form)
        pattern = f"{pdf_dir}/*{normalized_file}*.pdf"
        matching_files = glob.glob(pattern)

        if matching_files:
            break  # ë§¤ì¹­ë˜ë©´ ë” ì´ìƒ NFD/NFC ì‹œë„ ì•ˆ í•¨

    if matching_files:
        for pdf_path in matching_files:
            pdf_filename = os.path.basename(pdf_path)
            markdown_path = markdown_dir / pdf_filename.replace('.pdf', '.md')

            print(f"ğŸ” Found file: {pdf_filename} (Normalization: {norm_form})")

            if pdf_filename in processed_files:
                print(f"â© Already processed: {pdf_filename}")
                continue

            processed_files.append(pdf_filename)

            try:
                print(f"Processing: {pdf_filename}")
                success = convert_pdf_to_structured_markdown(Path(pdf_path), markdown_path)
                if success:
                    print(f"converted: {pdf_filename}")
                else:
                    print(f"Failed to convert: {pdf_filename}")
            except Exception as e:
                with error_log_path.open("a", encoding="utf-8") as error_file:
                    error_file.write(f"Error converting {pdf_filename}: {str(e)}\n")
                    error_file.write(traceback.format_exc())
                    error_file.write("\n" + "=" * 50 + "\n")
                print(f"Error converting {pdf_filename}. See log for details.")
    else:
        print(f"No matching file found for: {problem_file}")
        print(f"  - Tried patterns: {pdf_dir}/*{normalize_filename(file_base, 'NFC')}*.pdf")
        print(f"                  : {pdf_dir}/*{normalize_filename(file_base, 'NFD')}*.pdf")
        # ë””ë ‰í† ë¦¬ ë‚´ íŒŒì¼ ëª©ë¡ í™•ì¸
        all_files = glob.glob(f"{pdf_dir}/*.pdf")
        print(f"  - Files in directory: {[os.path.basename(f) for f in all_files]}")

print("ëª¨ë“  PDF íŒŒì¼ ë³€í™˜ ì™„ë£Œ")
